{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json, io, requests, string\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomDatasetFromImages(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            img_path (string): path to the folder where images are\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        # Transforms\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        # Read the csv file\n",
    "        self.data_info = pd.read_csv(csv_path, header=None)\n",
    "        # First column contains the image paths\n",
    "        self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n",
    "        # Second column is the labels\n",
    "        self.label_arr = np.asarray(self.data_info.iloc[:, 1])\n",
    "        # Third column is for an operation indicator\n",
    "        self.operation_arr = np.asarray(self.data_info.iloc[:, 2])\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.data_info.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_name = self.image_arr[index]\n",
    "        # Open image\n",
    "        img_as_img = Image.open(single_image_name)\n",
    "\n",
    "        # Check if there is an operation\n",
    "        some_operation = self.operation_arr[index]\n",
    "        # If there is an operation\n",
    "        if some_operation:\n",
    "            # Do some operation on image\n",
    "            # ...\n",
    "            # ...\n",
    "            pass\n",
    "        # Transform image to tensor\n",
    "        img_as_tensor = self.to_tensor(img_as_img)\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        single_image_label = self.label_arr[index]\n",
    "\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "if __name__ == \"__main__\":\n",
    "    # Define transforms\n",
    "    \n",
    "    # Define custom dataset\n",
    "    train_data = CustomDatasetFromImages(\"Train0.csv\")\n",
    "    validation_data = CustomDatasetFromImages(\"Val0.csv\")\n",
    "    \n",
    "    # Define data loader\n",
    "#     trainset = torch.utils.data.DataLoader(dataset=train_data,\n",
    "#                                                     batch_size=100,\n",
    "#                                                     shuffle=True)\n",
    "    \n",
    "#     valset = torch.utils.data.DataLoader(dataset=validation_data,\n",
    "#                                                     batch_size=100,\n",
    "#                                                     shuffle=False)\n",
    "    \n",
    "    #for images, labels in trainset:\n",
    "        # Feed the data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bachSize = 100\n",
    "class TwoLayerNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TwoLayerNN, self).__init__()\n",
    "  \n",
    "    self.linear1 = nn.Linear(3 * 224 * 224, 512)\n",
    "    self.linear2 = nn.Linear(512, 6)\n",
    "        \n",
    "  def forward(self, x):\n",
    "    x = x.view(x.size(0), 3 * 224 * 224)\n",
    "    z = F.relu(self.linear1(x))\n",
    "    return self.linear2(z)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNN()\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomDatasetFromImages'>\n",
      "328\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torch.optim as optim\n",
    "\n",
    "preprocessFn = transforms.Compose(\n",
    "    [transforms.Resize(224),  # 1. Resize smallest side to 256.\n",
    "     transforms.CenterCrop(224), # 2. Crop the center 224x224 pixels.\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean = [0.485, 0.456, 0.406],  # normalize.\n",
    "                          std = [0.229, 0.224, 0.225])])\n",
    "print(type(train_data))\n",
    "# img = []\n",
    "# labellist = []\n",
    "imgtensor = torch.zeros(1768,3,224,224)\n",
    "labeltensor = torch.LongTensor(1768,1).zero_()\n",
    "counter = 0\n",
    "for (i, (inputs, labels)) in enumerate(train_data):\n",
    "    counter += 1\n",
    "    art= transforms.ToPILImage()(inputs)\n",
    "    imgtensor[i,:,:,:] = preprocessFn(art).unsqueeze(0)\n",
    "#     print(labels)\n",
    "    labeltensor[i,:] = torch.tensor(labels)\n",
    "    \n",
    "imgtensorVal = torch.zeros(328,3,224,224)\n",
    "labeltensorVal = torch.LongTensor(328,1).zero_()\n",
    "counter = 0\n",
    "for (i, (inputs, labels)) in enumerate(validation_data):\n",
    "    counter += 1\n",
    "    art= transforms.ToPILImage()(inputs)\n",
    "    imgtensorVal[i,:,:,:] = preprocessFn(art).unsqueeze(0)\n",
    "#     print(labels)\n",
    "    labeltensorVal[i,:] = torch.tensor(labels)\n",
    "    \n",
    "print(counter)\n",
    "# Load the training, and validation datasets.\n",
    "trainset = train_data\n",
    "valset = validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 224, 224])\n",
      "Train-epoch 0. Iteration 00001, Avg-Loss: 1.8309, Accuracy: 16.8200\n",
      "Train-epoch 0. Iteration 00002, Avg-Loss: 2.6300, Accuracy: 20.5450\n",
      "Train-epoch 0. Iteration 00003, Avg-Loss: 2.4764, Accuracy: 20.7567\n",
      "Train-epoch 0. Iteration 00004, Avg-Loss: 2.3539, Accuracy: 20.2525\n",
      "Train-epoch 0. Iteration 00005, Avg-Loss: 2.2944, Accuracy: 19.3340\n",
      "Train-epoch 0. Iteration 00006, Avg-Loss: 2.2191, Accuracy: 18.8717\n",
      "Train-epoch 0. Iteration 00007, Avg-Loss: 2.1318, Accuracy: 19.0414\n",
      "Train-epoch 0. Iteration 00008, Avg-Loss: 2.0606, Accuracy: 18.8562\n",
      "Train-epoch 0. Iteration 00009, Avg-Loss: 2.0227, Accuracy: 18.8122\n",
      "Train-epoch 0. Iteration 00010, Avg-Loss: 1.9624, Accuracy: 19.2050\n",
      "Train-epoch 0. Iteration 00011, Avg-Loss: 1.9165, Accuracy: 19.1982\n",
      "Train-epoch 0. Iteration 00012, Avg-Loss: 1.8789, Accuracy: 19.2142\n",
      "Train-epoch 0. Iteration 00013, Avg-Loss: 1.8557, Accuracy: 19.2092\n",
      "Train-epoch 0. Iteration 00014, Avg-Loss: 1.8318, Accuracy: 19.5829\n",
      "Train-epoch 0. Iteration 00015, Avg-Loss: 1.8100, Accuracy: 19.6400\n",
      "Train-epoch 0. Iteration 00016, Avg-Loss: 1.7867, Accuracy: 19.6062\n",
      "Train-epoch 0. Iteration 00017, Avg-Loss: 1.7680, Accuracy: 19.6724\n",
      "Train-epoch 0. Iteration 00018, Avg-Loss: 1.7600, Accuracy: 19.0561\n",
      "Validation-epoch 0. Avg-Loss: 1.4968, Accuracy: 17.0518\n",
      "Train-epoch 1. Iteration 00001, Avg-Loss: 1.5088, Accuracy: 17.3200\n",
      "Train-epoch 1. Iteration 00002, Avg-Loss: 1.3728, Accuracy: 18.4500\n",
      "Train-epoch 1. Iteration 00003, Avg-Loss: 1.3221, Accuracy: 18.8600\n",
      "Train-epoch 1. Iteration 00004, Avg-Loss: 1.2899, Accuracy: 19.2625\n",
      "Train-epoch 1. Iteration 00005, Avg-Loss: 1.2875, Accuracy: 19.4880\n",
      "Train-epoch 1. Iteration 00006, Avg-Loss: 1.3005, Accuracy: 19.4033\n",
      "Train-epoch 1. Iteration 00007, Avg-Loss: 1.3104, Accuracy: 19.3357\n",
      "Train-epoch 1. Iteration 00008, Avg-Loss: 1.3272, Accuracy: 19.5350\n",
      "Train-epoch 1. Iteration 00009, Avg-Loss: 1.3208, Accuracy: 19.6756\n",
      "Train-epoch 1. Iteration 00010, Avg-Loss: 1.3084, Accuracy: 19.4420\n",
      "Train-epoch 1. Iteration 00011, Avg-Loss: 1.3046, Accuracy: 19.3964\n",
      "Train-epoch 1. Iteration 00012, Avg-Loss: 1.3094, Accuracy: 19.2525\n",
      "Train-epoch 1. Iteration 00013, Avg-Loss: 1.3156, Accuracy: 19.1723\n",
      "Train-epoch 1. Iteration 00014, Avg-Loss: 1.3084, Accuracy: 19.2321\n",
      "Train-epoch 1. Iteration 00015, Avg-Loss: 1.3043, Accuracy: 19.2620\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-79f0dfa34175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-79f0dfa34175>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_fn, batchSize, trainset, valset, optimizer)\u001b[0m\n\u001b[0;32m     54\u001b[0m           \u001b[1;31m#input_img =  preprocessFn(pil_img).unsqueeze(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m           \u001b[1;31m# Forward pass. (Prediction stage)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m           \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;31m#           print(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m           \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-344932c2b3b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m224\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# print(randomizer)\n",
    "\n",
    "train_accuracies = []; val_accuracies = []\n",
    "train_losses = []; val_losses = []\n",
    "\n",
    "trainset = torch.utils.data.TensorDataset(imgtensor, labeltensor)\n",
    "valset = torch.utils.data.TensorDataset(imgtensorVal, labeltensorVal)\n",
    "\n",
    "\n",
    "print(type(imgtensor))\n",
    "print(imgtensor[0].shape)\n",
    "\n",
    "def train_model(model, loss_fn, batchSize, trainset, valset, optimizer):\n",
    "  \n",
    "  # Shuffling is needed in case dataset is not shuffled by default.\n",
    "  train_loader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                              batch_size = batchSize,\n",
    "                                              shuffle = True)\n",
    "#   # We don't need to bach the validation set but let's do it anyway.\n",
    "  val_loader = torch.utils.data.DataLoader(dataset = valset,\n",
    "                                             batch_size = batchSize,\n",
    "                                             shuffle = False) # No need.\n",
    "  \n",
    "  # Define number of epochs.\n",
    "  N = 5\n",
    "\n",
    "  # log accuracies and losses.\n",
    "  train_accuracies = []; val_accuracies = []\n",
    "  train_losses = []; val_losses = []\n",
    "\n",
    "  # GPU enabling.\n",
    "#   model = model.cuda()\n",
    "#   loss_fn = loss_fn.cuda()\n",
    "\n",
    "\n",
    "  # Training loop. Please make sure you understand every single line of code below.\n",
    "  # Go back to some of the previous steps in this lab if necessary.\n",
    "  for epoch in range(0, N):\n",
    "      correct = 0.0\n",
    "      cum_loss = 0.0\n",
    "\n",
    "      # Make a pass over the training data.\n",
    "      model.train()\n",
    "      for (i, (inputs, labels)) in enumerate(train_loader):\n",
    "          #print(type(inputs))\n",
    "#           inputs = inputs.cuda()\n",
    "#           labels = labels.cuda()\n",
    "          #print(inputs.shape)\n",
    "          #trans = transforms.ToPILImage()\n",
    "          #img = transforms.ToPILImage()(inputs)\n",
    "          #pil_img = trans(inputs)\n",
    "          #input_img =  preprocessFn(pil_img).unsqueeze(0)\n",
    "          # Forward pass. (Prediction stage)\n",
    "          scores = model(inputs)\n",
    "#           print(labels)\n",
    "          loss = loss_fn(scores, labels.view(-1))\n",
    "#           print(labels.view(-1))\n",
    "#           print(scores.shape)\n",
    "          # Count how many correct in this batch.\n",
    "          max_scores, max_labels = scores.max(1)\n",
    "#           print(max_labels)\n",
    "#           print(labels.view(-1))\n",
    "          correct += (max_labels == labels).sum().item()\n",
    "          cum_loss += loss.item()\n",
    "\n",
    "          # Zero the gradients in the network.\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          #Backward pass. (Gradient computation stage)\n",
    "          loss.backward()\n",
    "\n",
    "          # Parameter updates (SGD step) -- if done with torch.optim!\n",
    "          optimizer.step()\n",
    "          # Parameter updates (SGD step) -- if done manually!\n",
    "          # for param in model.parameters():\n",
    "          #   param.data.add_(-learningRate, param.grad)\n",
    "\n",
    "          # Logging the current results on training.\n",
    "          if (i + 1) % 1 == 0:\n",
    "              print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "                    (epoch, i + 1, cum_loss / (i + 1), correct / ((i + 1) * batchSize)))\n",
    "\n",
    "\n",
    "      train_accuracies.append(correct / len(trainset))\n",
    "      train_losses.append(cum_loss / (i + 1))   \n",
    "\n",
    "      # Make a pass over the validation data.\n",
    "      correct = 0.0\n",
    "      cum_loss = 0.0\n",
    "      model.eval()\n",
    "      for (i, (inputs, labels)) in enumerate(val_loader):\n",
    "# #           inputs = inputs.cuda()\n",
    "# #           labels = labels.cuda()\n",
    "\n",
    "\n",
    "#           # Forward pass. (Prediction stage)\n",
    "           scores = model(inputs)\n",
    "           cum_loss += loss_fn(scores, labels.view(-1)).item()\n",
    "\n",
    "#            # Count how many correct in this batch.\n",
    "           max_scores, max_labels = scores.max(1)\n",
    "           correct += (max_labels == labels).sum().item()\n",
    "\n",
    "      val_accuracies.append(correct / len(valset))\n",
    "      val_losses.append(cum_loss / (i + 1))\n",
    "      \n",
    "      # Logging the current results on validation.\n",
    "      print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "            (epoch, cum_loss / (i + 1), correct / len(valset)))\n",
    "    \n",
    "  plt.figure(figsize = (10, 4))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(val_losses, 'bo-', label = 'val-loss')\n",
    "  plt.plot(train_losses, 'ro-', label = 'train-loss') \n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch') \n",
    "  plt.legend(['validation', 'training'], loc='upper right')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(val_accuracies, 'bo-', label = 'val-acc')\n",
    "  plt.plot(train_accuracies, 'ro-', label = 'train-acc')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['validation', 'training'], loc='lower right')\n",
    "  plt.show()      \n",
    "\n",
    "\n",
    "      \n",
    "model = TwoLayerNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "# Create the model.\n",
    "# Optimizer.\n",
    "learningRate = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr = learningRate)\n",
    "batchSize = 100\n",
    "\n",
    "train_model(model, loss_fn, batchSize, trainset, valset, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
